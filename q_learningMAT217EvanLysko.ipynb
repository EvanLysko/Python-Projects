{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"q_learningMAT217EvanLysko.ipynb","provenance":[],"authorship_tag":"ABX9TyP9QimUGWj3YEH6kvIriMSy"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"i5VWtR41Y6Ra"},"source":["Changing the walls in the maze"]},{"cell_type":"code","metadata":{"id":"VexkXYuH4ZcC","executionInfo":{"status":"ok","timestamp":1604811890631,"user_tz":300,"elapsed":635,"user":{"displayName":"Evan Lysko","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSgRtiFtZnbcU-5TbtLB-L0EH2j4lAyMMQ4UIQ=s64","userId":"13269421218988419811"}},"outputId":"968c9f79-70be-4e82-a15e-53ea5e54a370","colab":{"base_uri":"https://localhost:8080/"}},"source":["# AI for Logistics - Robots in a warehouse\n","\n","# Importing the libraries\n","import numpy as np\n","\n","# Setting the parameters gamma and alpha for the Q-Learning\n","gamma = 0.75\n","alpha = 0.9\n","\n","# PART 1 - BUILDING THE ENVIRONMENT\n","\n","# Defining the states\n","location_to_state = {'A': 0,\n","                     'B': 1,\n","                     'C': 2,\n","                     'D': 3,\n","                     'E': 4,\n","                     'F': 5,\n","                     'G': 6,\n","                     'H': 7,\n","                     'I': 8,\n","                     'J': 9,\n","                     'K': 10,\n","                     'L': 11}\n","\n","# Defining the actions\n","actions = [0,1,2,3,4,5,6,7,8,9,10,11]\n","\n","# Defining the rewards\n","R = np.array([[0,0,0,0,1,0,0,0,0,0,0,0],\n","              [0,0,1,0,0,1,0,0,0,0,0,0],\n","              [0,1,0,1,0,0,0,0,0,0,0,0],\n","              [0,0,1,0,0,0,0,0,0,0,0,0],\n","              [1,0,0,0,0,1,0,0,1,0,0,0],\n","              [0,1,0,0,1,0,0,0,0,0,0,0],\n","              [0,0,0,0,0,0,0,1,0,0,1,0],\n","              [0,0,0,0,0,0,1,0,0,0,0,0],\n","              [0,0,0,0,1,0,0,0,0,1,0,0],\n","              [0,0,0,0,0,0,0,0,1,0,1,0],\n","              [0,0,0,0,0,0,1,0,0,1,0,1],\n","              [0,0,0,0,0,0,0,0,0,0,1,0]])\n","\n","# PART 2 - BUILDING THE AI SOLUTION WITH Q-LEARNING\n","\n","# Making a mapping from the states to the locations\n","state_to_location = {state: location for location, state in location_to_state.items()}\n","\n","# Making a function that returns the shortest route from a starting to ending location\n","def route(starting_location, ending_location):\n","    R_new = np.copy(R)\n","    ending_state = location_to_state[ending_location]\n","    R_new[ending_state, ending_state] = 1000\n","    Q = np.array(np.zeros([12,12]))\n","    for i in range(1000):\n","        current_state = np.random.randint(0,12)\n","        playable_actions = []\n","        for j in range(12):\n","            if R_new[current_state, j] > 0:\n","                playable_actions.append(j)\n","        next_state = np.random.choice(playable_actions)\n","        TD = R_new[current_state, next_state] + gamma * Q[next_state, np.argmax(Q[next_state,])] - Q[current_state, next_state]\n","        Q[current_state, next_state] = Q[current_state, next_state] + alpha * TD\n","    route = [starting_location]\n","    next_location = starting_location\n","    while (next_location != ending_location):\n","        starting_state = location_to_state[starting_location]\n","        next_state = np.argmax(Q[starting_state,])\n","        next_location = state_to_location[next_state]\n","        route.append(next_location)\n","        starting_location = next_location\n","    return route\n","\n","# PART 3 - GOING INTO PRODUCTION\n","\n","# Printing the final route\n","print('Route:')\n","route('A', 'H')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Route:\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['A', 'E', 'I', 'J', 'K', 'G', 'H']"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"P3XrL-3DZCcG"},"source":["Expanding the Maze"]},{"cell_type":"code","metadata":{"id":"ogsJgk6UZGjD","executionInfo":{"status":"ok","timestamp":1604818759972,"user_tz":300,"elapsed":639,"user":{"displayName":"Evan Lysko","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSgRtiFtZnbcU-5TbtLB-L0EH2j4lAyMMQ4UIQ=s64","userId":"13269421218988419811"}},"outputId":"c728537d-51df-4c7b-f02e-62b7eb5abc1e","colab":{"base_uri":"https://localhost:8080/"}},"source":["# AI for Logistics - Robots in a warehouse\n","\n","# Importing the libraries\n","import numpy as np\n","\n","# Setting the parameters gamma and alpha for the Q-Learning\n","gamma = 0.75\n","alpha = 0.9\n","\n","# PART 1 - BUILDING THE ENVIRONMENT\n","\n","# Defining the states\n","location_to_state = {'A': 0,\n","                     'B': 1,\n","                     'C': 2,\n","                     'D': 3,\n","                     'E': 4,\n","                     'F': 5,\n","                     'G': 6,\n","                     'H': 7,\n","                     'I': 8,\n","                     'J': 9,\n","                     'K': 10,\n","                     'L': 11,\n","                     'M': 12,\n","                     'N': 13,\n","                     'O': 14,\n","                     'P': 15}\n","\n","# Defining the actions\n","actions = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n","\n","# Defining the rewards\n","R = np.array([[0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0],\n","              [0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0],\n","              [0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0],\n","              [0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n","              [1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0],\n","              [0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0],\n","              [0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0],\n","              [0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0],\n","              [0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0],\n","              [0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0],\n","              [0,0,0,0,0,0,1,0,0,1,0,1,0,0,1,0],\n","              [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],\n","              [0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0],\n","              [0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0],\n","              [0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1],\n","              [0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0]])\n","\n","# PART 2 - BUILDING THE AI SOLUTION WITH Q-LEARNING\n","\n","# Making a mapping from the states to the locations\n","state_to_location = {state: location for location, state in location_to_state.items()}\n","\n","# Making a function that returns the shortest route from a starting to ending location\n","def route(starting_location, ending_location):\n","    R_new = np.copy(R)\n","    ending_state = location_to_state[ending_location]\n","    R_new[ending_state, ending_state] = 1000\n","    Q = np.array(np.zeros([16,16]))\n","    for i in range(1000):\n","        current_state = np.random.randint(0,15)\n","        playable_actions = []\n","        for j in range(16):\n","            if R_new[current_state, j] > 0:\n","                playable_actions.append(j)\n","        next_state = np.random.choice(playable_actions)\n","        TD = R_new[current_state, next_state] + gamma * Q[next_state, np.argmax(Q[next_state,])] - Q[current_state, next_state]\n","        Q[current_state, next_state] = Q[current_state, next_state] + alpha * TD\n","    route = [starting_location]\n","    next_location = starting_location\n","    while (next_location != ending_location):\n","        starting_state = location_to_state[starting_location]\n","        next_state = np.argmax(Q[starting_state,])\n","        next_location = state_to_location[next_state]\n","        route.append(next_location)\n","        starting_location = next_location\n","    return route\n","\n","# PART 3 - GOING INTO PRODUCTION\n","\n","# Printing the final route\n","print('Route:')\n","route('A', 'L')"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Route:\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['A', 'E', 'I', 'J', 'K', 'L']"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"Y7m0J_BIn9wM"},"source":["Intermediate Goals. The code provided uses intermiate goals just by running the main function twice instead of doing a 500 and 1000 reward so I just went along with that."]},{"cell_type":"code","metadata":{"id":"JAXgE8kpoDQv","executionInfo":{"status":"ok","timestamp":1604818769329,"user_tz":300,"elapsed":1208,"user":{"displayName":"Evan Lysko","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSgRtiFtZnbcU-5TbtLB-L0EH2j4lAyMMQ4UIQ=s64","userId":"13269421218988419811"}},"outputId":"95a2e63d-95e8-4a41-c5ed-3260a5ec650e","colab":{"base_uri":"https://localhost:8080/"}},"source":["# AI for Logistics - Robots in a warehouse\n","\n","# Importing the libraries\n","import numpy as np\n","\n","# Setting the parameters gamma and alpha for the Q-Learning\n","gamma = 0.75\n","alpha = 0.9\n","\n","# PART 1 - BUILDING THE ENVIRONMENT\n","\n","# Defining the states\n","location_to_state = {'A': 0,\n","                     'B': 1,\n","                     'C': 2,\n","                     'D': 3,\n","                     'E': 4,\n","                     'F': 5,\n","                     'G': 6,\n","                     'H': 7,\n","                     'I': 8,\n","                     'J': 9,\n","                     'K': 10,\n","                     'L': 11,\n","                     'M': 12,\n","                     'N': 13,\n","                     'O': 14,\n","                     'P': 15}\n","\n","# Defining the actions\n","actions = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n","\n","# Defining the rewards\n","R = np.array([[0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0],\n","              [0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0],\n","              [0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0],\n","              [0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n","              [1,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0],\n","              [0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0],\n","              [0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0],\n","              [0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0],\n","              [0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0],\n","              [0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0],\n","              [0,0,0,0,0,0,1,0,0,1,0,1,0,0,1,0],\n","              [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],\n","              [0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0],\n","              [0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0],\n","              [0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1],\n","              [0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0]])\n","\n","# PART 2 - BUILDING THE AI SOLUTION WITH Q-LEARNING\n","\n","# Making a mapping from the states to the locations\n","state_to_location = {state: location for location, state in location_to_state.items()}\n","\n","# Making a function that returns the shortest route from a starting to ending location\n","def route(starting_location, ending_location):\n","    R_new = np.copy(R)\n","    ending_state = location_to_state[ending_location]\n","    R_new[ending_state, ending_state] = 1000\n","    Q = np.array(np.zeros([16,16]))\n","    for i in range(1000):\n","        current_state = np.random.randint(0,15)\n","        playable_actions = []\n","        for j in range(16):\n","            if R_new[current_state, j] > 0:\n","                playable_actions.append(j)\n","        next_state = np.random.choice(playable_actions)\n","        TD = R_new[current_state, next_state] + gamma * Q[next_state, np.argmax(Q[next_state,])] - Q[current_state, next_state]\n","        Q[current_state, next_state] = Q[current_state, next_state] + alpha * TD\n","    route = [starting_location]\n","    next_location = starting_location\n","    while (next_location != ending_location):\n","        starting_state = location_to_state[starting_location]\n","        next_state = np.argmax(Q[starting_state,])\n","        next_location = state_to_location[next_state]\n","        route.append(next_location)\n","        starting_location = next_location\n","    return route\n","\n","# PART 3 - GOING INTO PRODUCTION\n","\n","# Making the final function that returns the optimal route\n","def best_route(starting_location, intermediary_location, ending_location):\n","    return route(starting_location, intermediary_location) + route(intermediary_location, ending_location)[1:]\n","\n","# Printing the final route\n","print('Route:')\n","best_route('D', 'K', 'H')"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Route:\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['D', 'C', 'B', 'F', 'E', 'I', 'J', 'K', 'G', 'H']"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"Ew4whvheoERv"},"source":["Modified maze to make choice of route varied. made it avoid F"]},{"cell_type":"code","metadata":{"id":"8DjgBPyAp1_J","executionInfo":{"status":"ok","timestamp":1604819110173,"user_tz":300,"elapsed":712,"user":{"displayName":"Evan Lysko","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiSgRtiFtZnbcU-5TbtLB-L0EH2j4lAyMMQ4UIQ=s64","userId":"13269421218988419811"}},"outputId":"365c3b75-1ddf-4306-c978-b84fe4e34ff9","colab":{"base_uri":"https://localhost:8080/"}},"source":["# AI for Logistics - Robots in a warehouse\n","\n","# Importing the libraries\n","import numpy as np\n","\n","# Setting the parameters gamma and alpha for the Q-Learning\n","gamma = 0.75\n","alpha = 0.9\n","\n","# PART 1 - BUILDING THE ENVIRONMENT\n","\n","# Defining the states\n","location_to_state = {'A': 0,\n","                     'B': 1,\n","                     'C': 2,\n","                     'D': 3,\n","                     'E': 4,\n","                     'F': 5,\n","                     'G': 6,\n","                     'H': 7,\n","                     'I': 8,\n","                     'J': 9,\n","                     'K': 10,\n","                     'L': 11,\n","                     'M': 12,\n","                     'N': 13,\n","                     'O': 14,\n","                     'P': 15}\n","\n","# Defining the actions\n","actions = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n","\n","# Defining the rewards\n","R = np.array([[0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0],\n","              [1,0,1,0,0,-500,0,0,0,0,0,0,0,0,0,0],\n","              [0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0],\n","              [0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0],\n","              [1,0,0,0,0,-500,0,0,1,0,0,0,0,0,0,0],\n","              [0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0],\n","              [0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0],\n","              [0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0],\n","              [0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0],\n","              [0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0],\n","              [0,0,0,0,0,0,1,0,0,1,0,1,0,0,1,0],\n","              [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1],\n","              [0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0],\n","              [0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0],\n","              [0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1],\n","              [0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0]])\n","\n","# PART 2 - BUILDING THE AI SOLUTION WITH Q-LEARNING\n","\n","# Making a mapping from the states to the locations\n","state_to_location = {state: location for location, state in location_to_state.items()}\n","\n","# Making a function that returns the shortest route from a starting to ending location\n","def route(starting_location, ending_location):\n","    R_new = np.copy(R)\n","    ending_state = location_to_state[ending_location]\n","    R_new[ending_state, ending_state] = 1000\n","    Q = np.array(np.zeros([16,16]))\n","    for i in range(1000):\n","        current_state = np.random.randint(0,15)\n","        playable_actions = []\n","        for j in range(16):\n","            if R_new[current_state, j] > 0:\n","                playable_actions.append(j)\n","        next_state = np.random.choice(playable_actions)\n","        TD = R_new[current_state, next_state] + gamma * Q[next_state, np.argmax(Q[next_state,])] - Q[current_state, next_state]\n","        Q[current_state, next_state] = Q[current_state, next_state] + alpha * TD\n","    route = [starting_location]\n","    next_location = starting_location\n","    while (next_location != ending_location):\n","        starting_state = location_to_state[starting_location]\n","        next_state = np.argmax(Q[starting_state,])\n","        next_location = state_to_location[next_state]\n","        route.append(next_location)\n","        starting_location = next_location\n","    return route\n","\n","# PART 3 - GOING INTO PRODUCTION\n","\n","# Making the final function that returns the optimal route\n","def best_route(starting_location, intermediary_location, ending_location):\n","    return route(starting_location, intermediary_location) + route(intermediary_location, ending_location)[1:]\n","\n","# Printing the final route\n","print('Route:')\n","best_route('D', 'K', 'H')"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Route:\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['D', 'C', 'B', 'A', 'E', 'I', 'J', 'K', 'G', 'H']"]},"metadata":{"tags":[]},"execution_count":14}]}]}